{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchies in BigARTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Method explaination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usual ARTM model\n",
    "__Data:__ documents set $D$, words set $W$, document-word matrix $\\{n_{dw}\\}_{D \\times W}$. \n",
    "\n",
    "__Model:__ Denote $p(w|d) = \\frac{n_{dw}}{\\sum_w n_{dw}}$, $T$ is a topics set. The topic model is\n",
    "$$ p(w|d) = \\sum_{t \\in T} p(w|t) p(t|d) = \\sum_{t \\in T} \\phi_{wt} \\theta_{td}, \\hspace{3cm} (1) $$\n",
    "with parameters\n",
    "\n",
    "* $\\Phi = \\{\\phi_{wt}\\}_{W \\times T}$\n",
    "* $\\Theta = \\{ \\theta_{td}\\}_{T \\times D}$\n",
    "\n",
    "__Parameter learning:__ regularizer maximum likelihood maximization\n",
    "$$ \\sum_d \\sum_w n_{dw} \\ln \\sum_t \\phi_{wt} \\theta_{td} + \\sum_i \\tau_i R_i(\\Phi, \\Theta) \\rightarrow max_{\\Phi, \\Theta} $$\n",
    "where regularizers $R(\\Phi, \\Theta) = \\sum_i \\tau_i R_i(\\Phi, \\Theta)$ allows introducing additional subject-specific criterias, $\\tau_i$ are regularizers' coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How hierarchy is constructed from several usual models\n",
    "#### Hierarchy definition:\n",
    "* __Topic hierarchy__ is an oriented multipartite (multilevel) graph of topics so that edges connect only topics from neighboring levels. \n",
    "* Zero level consists of the only node called __root__.\n",
    "* Each none-zero level has more topics than previous one. Previous level is called __parent level__.\n",
    "* If there is edge topic-subtopic in hierarchy, topic is also called __parent topic__ or __ancestor__. \n",
    "\n",
    "#### Hierarchy construction:\n",
    "* Root is associated with the whole collection and doesn't need modeling.\n",
    "* _Every non-zero level is a usual topic model._\n",
    "* First level has few topics that are main collection topics. First level topics have the only parent topic (root). \n",
    "* For each level with index > 1 we need to to establish parent-children relationship with previous level topics.\n",
    "\n",
    "### Establishing parent-children relations\n",
    "When we built parent level, let's denote its topics $a \\in A$ (ancestor) and matrices $\\Phi^p$ and $\\theta^p$.\n",
    "\n",
    "Let's introduce new matrix factorization problem:\n",
    "    $$ \\phi^p_{wa} = p(w|a) \\approx \\sum_{t} p(w|t) p(t|a) = \\sum_t \\phi_{wt} \\psi_{ta}$$    \n",
    "    \n",
    "with new parameters $\\Psi = \\{ \\psi_{ta} \\}_{T \\times A}$.\n",
    "\n",
    "If KL-divergence is a similarity measure between distributions, then we can create regularizer:\n",
    "   $$ R(\\Phi, \\Psi) = \\sum_w \\sum_a \\phi_{wa} \\ln \\sum_t \\phi_{wt} \\psi_{ta} \\rightarrow max_{\\Phi, \\Psi}  $$.\n",
    "\n",
    "   $$ \\sum_d \\sum_w n_{dw} \\ln \\sum_t \\phi_{wt} \\theta_{td} + \\tau  R(\\Phi, \\Psi) \\rightarrow max_{\\Phi, \\Psi, \\Theta}  $$\n",
    "Both likelihood and regularizer formulas have common structure. So there is a simple way to train $\\Psi$ simultaneously with $\\Phi$ and $\\Theta$:\n",
    "\n",
    "_we just add $|A|$ pseudodocuments to collection, each representing parent $\\Phi$ column: $n_{aw} = \\tau p(w|a)$._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. BigARTM implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchy in BigARTM is implemented in hierarchy_utils module. To build hierarchy, create hARTM instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from hierarchy_utils import hARTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hier = hARTM(self, num_processors=None, class_ids=None,\n",
    "             scores=None, regularizers=None, num_document_passes=10, reuse_theta=False,\n",
    "             dictionary=None, cache_theta=False, theta_columns_naming='id', seed=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should pass to hARTM parameters that are common for all levels. These are the same parameters that you pass to usual ARTM model.\n",
    "\n",
    "Levels will built one by one. To add first level, use add_level method specifying remaining model parameters (unique for the level):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "level0 = hier.add_level(self, num_topics=None, topic_names=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method returns ARTM object so you can work with it as you used: initialize it, fit offline, add regularizer ans scores etc. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_vectorizer = artm.BatchVectorizer(data_path=\"./my_batches\", data_format='batches')\n",
    "dictionary = artm.Dictionary('dictionary')\n",
    "dictionary.gather(batch_vectorizer.data_path)\n",
    "level0.initialize(dictionary=dictionary)\n",
    "level0.fit_offline(batch_vectorizer, num_collection_passes=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When first level is fit, you have to add next level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "level1 = hier.add_level(self, num_topics=None, topic_names=None, \n",
    "                        parent_level_weight=1, tmp_files_path=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you add this level, parent levels phi matrix will be saved into special, parent level batch.\n",
    "It is the way how pseudoduments are created.\n",
    "This created batch will be added to other batches when you fit model.\n",
    "Explaination of add_level parameters:\n",
    "* parent_level_weight is regularizer's coefficient $\\tau$. Token_values in parent level batch will be multiplied by parent_level_weight during learning.\n",
    "* tmp_files_path is a path where model can save this parent level batch.\n",
    "\n",
    "These two parameters are ignored during creation of first level.\n",
    "\n",
    "Now you can learn level1 model by any means. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "level1.initialize(dictionary=dictionary)\n",
    "level1.fit_offline(batch_vectorizer, num_collection_passes=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The part of $\\Theta$ matrix corresponding to parent level batch is $\\Psi$ matrix. To get it, use get_psi method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "psi = level1.get_psi()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note than level0 has no get_psi method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get levels specifying level_index (from 0 as usually in python so first level has index 0):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "some_level = hier.get_level(level_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To delete level, use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hier.del_level(level_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Be careful:__ if you delete not the last level, all next levels will be deleted too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save hierarchy when it is built use save method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hier.save(path, model_name=\"p_wt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here path is a path where you want to save hierarchy's files, model_name specifies what matrix to save (as in ARTM.save method)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load hierarchy, use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hier = hARTM(self, num_processors=None, class_ids=None,\n",
    "             scores=None, regularizers=None, num_document_passes=10, reuse_theta=False,\n",
    "             dictionary=None, cache_theta=False, theta_columns_naming='id', seed=-1)\n",
    "hier.load(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
